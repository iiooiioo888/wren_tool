"""
æœ¬åœ°ç·©å­˜ç³»çµ±
æä¾›é«˜æ•ˆçš„æ•¸æ“šç·©å­˜æ©Ÿåˆ¶ï¼Œæ¸›å°‘é‡è¤‡è¨ˆç®—å’Œæ•¸æ“šè¼‰å…¥æ™‚é–“
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from datetime import datetime, timedelta
import json
import pickle
import logging
import hashlib
import time
import threading
from dataclasses import dataclass, asdict
import os
import shutil

# è¨­å®šæ—¥èªŒ
logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """ç·©å­˜æ¢ç›®"""
    key: str
    data_hash: str
    file_path: str
    created_at: str
    last_accessed: str
    access_count: int
    size_bytes: int
    ttl_seconds: int
    metadata: Dict[str, Any]

    def is_expired(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦éæœŸ"""
        created_time = datetime.fromisoformat(self.created_at)
        expiry_time = created_time + timedelta(seconds=self.ttl_seconds)
        return datetime.now() > expiry_time

    def update_access(self):
        """æ›´æ–°è¨ªå•çµ±è¨ˆ"""
        self.last_accessed = datetime.now().isoformat()
        self.access_count += 1

class CacheManager:
    """ç·©å­˜ç®¡ç†å™¨"""

    def __init__(
        self,
        cache_dir: str = "data/storage/cache",
        max_size_gb: float = 1.0,
        default_ttl: int = 3600
    ):
        """
        åˆå§‹åŒ–ç·©å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç·©å­˜ç›®éŒ„è·¯å¾‘
            max_size_gb: æœ€å¤§ç·©å­˜å¤§å°ï¼ˆGBï¼‰
            default_ttl: é è¨­ç·©å­˜æ™‚é–“ï¼ˆç§’ï¼‰
        """
        self.cache_dir = Path(cache_dir)
        self.max_size_bytes = max_size_gb * 1024 * 1024 * 1024
        self.default_ttl = default_ttl

        # å‰µå»ºç·©å­˜ç›®éŒ„
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # ç·©å­˜ç´¢å¼•æª”æ¡ˆ
        self.index_file = self.cache_dir / 'cache_index.json'

        # è¼‰å…¥ç¾æœ‰ç·©å­˜ç´¢å¼•
        self.cache_index = self._load_cache_index()

        # é–å®šæ©Ÿåˆ¶ç¢ºä¿ç·šç¨‹å®‰å…¨
        self.lock = threading.Lock()

        # å•Ÿå‹•å®šæœŸæ¸…ç†ç·šç¨‹
        self.cleanup_thread = None
        self.start_cleanup_thread()

        logger.info(f"Cache manager initialized: {cache_dir}, max_size={max_size_gb}GB")

    def _load_cache_index(self) -> Dict[str, CacheEntry]:
        """è¼‰å…¥ç·©å­˜ç´¢å¼•"""
        if self.index_file.exists():
            try:
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # å°‡å­—å…¸è½‰æ›ç‚ºCacheEntryå°è±¡
                index = {}
                for key, entry_dict in data.items():
                    index[key] = CacheEntry(**entry_dict)

                logger.info(f"Loaded cache index with {len(index)} entries")
                return index

            except Exception as e:
                logger.error(f"Error loading cache index: {e}")

        return {}

    def _save_cache_index(self):
        """ä¿å­˜ç·©å­˜ç´¢å¼•"""
        try:
            # å°‡CacheEntryå°è±¡è½‰æ›ç‚ºå­—å…¸
            data = {key: asdict(entry) for key, entry in self.cache_index.items()}

            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            logger.error(f"Error saving cache index: {e}")

    def _generate_cache_key(self, data: Any, cache_type: str = "data") -> str:
        """ç”Ÿæˆç·©å­˜éµ"""
        # å‰µå»ºæ•¸æ“šå…§å®¹çš„å“ˆå¸Œå€¼ä½œç‚ºéµ
        if isinstance(data, pd.DataFrame):
            # å°æ–¼DataFrameï¼Œä½¿ç”¨å…§å®¹å’Œæ¬„ä½åç¨±ç”Ÿæˆå“ˆå¸Œ
            content = str(data.values.tobytes()) + str(sorted(data.columns.tolist()))
        elif isinstance(data, dict):
            content = json.dumps(data, sort_keys=True)
        elif isinstance(data, str):
            content = data
        else:
            content = str(data)

        # æ·»åŠ é¡å‹å‰ç¶´
        full_content = f"{cache_type}:{content}"
        return hashlib.sha256(full_content.encode()).hexdigest()

    def _get_cache_filepath(self, cache_key: str) -> Path:
        """ç²å–ç·©å­˜æª”æ¡ˆè·¯å¾‘"""
        # ä½¿ç”¨å‰ç¶´çµ„ç¹”æª”æ¡ˆçµæ§‹
        prefix = cache_key[:2]
        cache_subdir = self.cache_dir / prefix
        cache_subdir.mkdir(exist_ok=True)

        return cache_subdir / f"{cache_key}.pkl"

    def _get_current_cache_size(self) -> int:
        """ç²å–ç•¶å‰ç·©å­˜ç¸½å¤§å°"""
        total_size = 0

        for entry in self.cache_index.values():
            if Path(entry.file_path).exists():
                try:
                    total_size += entry.size_bytes
                except Exception:
                    pass

        return total_size

    def _cleanup_expired_entries(self) -> int:
        """æ¸…ç†éæœŸæ¢ç›®"""
        expired_keys = []
        current_time = datetime.now()

        for key, entry in self.cache_index.items():
            if entry.is_expired():
                expired_keys.append(key)

        # åˆªé™¤éæœŸæª”æ¡ˆå’Œç´¢å¼•
        for key in expired_keys:
            try:
                entry = self.cache_index[key]
                cache_file = Path(entry.file_path)

                if cache_file.exists():
                    cache_file.unlink()
                    logger.debug(f"Deleted expired cache file: {cache_file}")

                del self.cache_index[key]

            except Exception as e:
                logger.error(f"Error deleting expired cache entry {key}: {e}")

        if expired_keys:
            logger.info(f"Cleaned up {len(expired_keys)} expired cache entries")
            self._save_cache_index()

        return len(expired_keys)

    def _evict_lru_entries(self, target_bytes: int) -> int:
        """é©…é€æœ€å°‘ä½¿ç”¨çš„æ¢ç›®ï¼ˆLRUï¼‰"""
        if not self.cache_index:
            return 0

        # æŒ‰æœ€å¾Œè¨ªå•æ™‚é–“å’Œè¨ªå•æ¬¡æ•¸æ’åº
        entries_by_lru = sorted(
            self.cache_index.values(),
            key=lambda x: (x.last_accessed, x.access_count)
        )

        evicted_count = 0
        freed_bytes = 0

        for entry in entries_by_lru:
            if freed_bytes >= target_bytes:
                break

            try:
                # åˆªé™¤æª”æ¡ˆ
                cache_file = Path(entry.file_path)
                if cache_file.exists():
                    file_size = cache_file.stat().st_size
                    cache_file.unlink()
                    freed_bytes += file_size
                    evicted_count += 1

                # å¾ç´¢å¼•ä¸­ç§»é™¤
                del self.cache_index[entry.key]

            except Exception as e:
                logger.error(f"Error evicting cache entry {entry.key}: {e}")

        if evicted_count > 0:
            logger.info(f"Evicted {evicted_count} LRU cache entries, freed {freed_bytes} bytes")
            self._save_cache_index()

        return evicted_count

    def _ensure_cache_space(self, required_bytes: int):
        """ç¢ºä¿æœ‰è¶³å¤ çš„ç·©å­˜ç©ºé–“"""
        current_size = self._get_current_cache_size()

        if current_size + required_bytes > self.max_size_bytes:
            # éœ€è¦é‡‹æ”¾ç©ºé–“
            space_to_free = (current_size + required_bytes) - self.max_size_bytes

            # å…ˆæ¸…ç†éæœŸæ¢ç›®
            freed_by_expiry = 0
            expired_count = self._cleanup_expired_entries()

            if expired_count > 0:
                current_size = self._get_current_cache_size()
                freed_by_expiry = current_size

            # å¦‚æœé‚„éœ€è¦ç©ºé–“ï¼Œä½¿ç”¨LRUé©…é€
            remaining_to_free = space_to_free - freed_by_expiry
            if remaining_to_free > 0:
                self._evict_lru_entries(remaining_to_free)

    def set(
        self,
        key: str,
        data: Any,
        ttl_seconds: int = None,
        metadata: Dict[str, Any] = None
    ) -> bool:
        """
        è¨­å®šç·©å­˜

        Args:
            key: ç·©å­˜éµ
            data: è¦ç·©å­˜çš„æ•¸æ“š
            ttl_seconds: ç·©å­˜æ™‚é–“ï¼ˆç§’ï¼‰
            metadata: å…ƒæ•¸æ“š

        Returns:
            æ˜¯å¦æˆåŠŸè¨­å®šç·©å­˜
        """
        with self.lock:
            try:
                if ttl_seconds is None:
                    ttl_seconds = self.default_ttl

                if metadata is None:
                    metadata = {}

                # åºåˆ—åŒ–æ•¸æ“šä»¥è¨ˆç®—å¤§å°
                temp_file = self.cache_dir / f"temp_{key}.pkl"
                with open(temp_file, 'wb') as f:
                    pickle.dump(data, f)

                file_size = temp_file.stat().st_size

                # ç¢ºä¿æœ‰è¶³å¤ ç©ºé–“
                self._ensure_cache_space(file_size)

                # ç”Ÿæˆæœ€çµ‚ç·©å­˜éµå’Œæª”æ¡ˆè·¯å¾‘
                cache_key = hashlib.sha256(key.encode()).hexdigest()
                cache_filepath = self._get_cache_filepath(cache_key)

                # ç§»å‹•æª”æ¡ˆåˆ°æœ€çµ‚ä½ç½®
                shutil.move(str(temp_file), cache_filepath)

                # å‰µå»ºç·©å­˜æ¢ç›®
                cache_entry = CacheEntry(
                    key=cache_key,
                    data_hash=self._generate_cache_key(data),
                    file_path=str(cache_filepath),
                    created_at=datetime.now().isoformat(),
                    last_accessed=datetime.now().isoformat(),
                    access_count=0,
                    size_bytes=file_size,
                    ttl_seconds=ttl_seconds,
                    metadata=metadata
                )

                # æ·»åŠ åˆ°ç´¢å¼•
                self.cache_index[cache_key] = cache_entry
                self._save_cache_index()

                logger.debug(f"Cached data with key: {cache_key} ({file_size} bytes)")
                return True

            except Exception as e:
                logger.error(f"Error setting cache for key {key}: {e}")
                # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
                if temp_file.exists():
                    temp_file.unlink()
                return False

    def get(self, key: str) -> Any:
        """
        ç²å–ç·©å­˜æ•¸æ“š

        Args:
            key: ç·©å­˜éµ

        Returns:
            ç·©å­˜çš„æ•¸æ“šï¼Œå¦‚æœä¸å­˜åœ¨æˆ–éæœŸå‰‡è¿”å›None
        """
        with self.lock:
            cache_key = hashlib.sha256(key.encode()).hexdigest()

            # æª¢æŸ¥ç·©å­˜ç´¢å¼•
            if cache_key not in self.cache_index:
                return None

            entry = self.cache_index[cache_key]

            # æª¢æŸ¥æ˜¯å¦éæœŸ
            if entry.is_expired():
                logger.debug(f"Cache entry expired: {cache_key}")
                self.delete(cache_key)
                return None

            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            cache_file = Path(entry.file_path)
            if not cache_file.exists():
                logger.warning(f"Cache file missing: {cache_file}")
                del self.cache_index[cache_key]
                self._save_cache_index()
                return None

            try:
                # è¼‰å…¥æ•¸æ“š
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)

                # æ›´æ–°è¨ªå•çµ±è¨ˆ
                entry.update_access()
                self._save_cache_index()

                logger.debug(f"Cache hit for key: {cache_key}")
                return data

            except Exception as e:
                logger.error(f"Error loading cache for key {cache_key}: {e}")
                self.delete(cache_key)
                return None

    def delete(self, key: str) -> bool:
        """
        åˆªé™¤ç·©å­˜æ¢ç›®

        Args:
            key: ç·©å­˜éµ

        Returns:
            æ˜¯å¦æˆåŠŸåˆªé™¤
        """
        with self.lock:
            cache_key = hashlib.sha256(key.encode()).hexdigest()

            if cache_key not in self.cache_index:
                return False

            try:
                entry = self.cache_index[cache_key]

                # åˆªé™¤æª”æ¡ˆ
                cache_file = Path(entry.file_path)
                if cache_file.exists():
                    cache_file.unlink()

                # å¾ç´¢å¼•ä¸­ç§»é™¤
                del self.cache_index[cache_key]
                self._save_cache_index()

                logger.debug(f"Deleted cache entry: {cache_key}")
                return True

            except Exception as e:
                logger.error(f"Error deleting cache entry {cache_key}: {e}")
                return False

    def exists(self, key: str) -> bool:
        """
        æª¢æŸ¥ç·©å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ

        Args:
            key: ç·©å­˜éµ

        Returns:
            æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        """
        with self.lock:
            cache_key = hashlib.sha256(key.encode()).hexdigest()

            if cache_key not in self.cache_index:
                return False

            entry = self.cache_index[cache_key]

            # æª¢æŸ¥æ˜¯å¦éæœŸ
            if entry.is_expired():
                return False

            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
            return Path(entry.file_path).exists()

    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç·©å­˜"""
        with self.lock:
            try:
                # åˆªé™¤æ‰€æœ‰ç·©å­˜æª”æ¡ˆ
                for entry in self.cache_index.values():
                    cache_file = Path(entry.file_path)
                    if cache_file.exists():
                        cache_file.unlink()

                # æ¸…ç©ºç´¢å¼•
                self.cache_index.clear()
                self._save_cache_index()

                logger.info("All cache cleared")

            except Exception as e:
                logger.error(f"Error clearing cache: {e}")

    def get_stats(self) -> Dict[str, Any]:
        """ç²å–ç·©å­˜çµ±è¨ˆ"""
        with self.lock:
            total_size = sum(entry.size_bytes for entry in self.cache_index.values())
            active_entries = sum(1 for entry in self.cache_index.values() if not entry.is_expired())

            # è¨ˆç®—å‘½ä¸­ç‡ï¼ˆåŸºæ–¼è¨ªå•æ¬¡æ•¸ï¼‰
            total_accesses = sum(entry.access_count for entry in self.cache_index.values())

            return {
                'total_entries': len(self.cache_index),
                'active_entries': active_entries,
                'total_size_bytes': total_size,
                'total_size_mb': total_size / (1024 * 1024),
                'max_size_bytes': self.max_size_bytes,
                'max_size_gb': self.max_size_bytes / (1024 * 1024 * 1024),
                'utilization_pct': (total_size / self.max_size_bytes) * 100 if self.max_size_bytes > 0 else 0,
                'total_accesses': total_accesses,
                'cache_dir': str(self.cache_dir)
            }

    def start_cleanup_thread(self, interval_seconds: int = 300):
        """å•Ÿå‹•å®šæœŸæ¸…ç†ç·šç¨‹"""
        if self.cleanup_thread and self.cleanup_thread.is_alive():
            return

        def cleanup_worker():
            while True:
                try:
                    time.sleep(interval_seconds)
                    self._cleanup_expired_entries()
                except Exception as e:
                    logger.error(f"Error in cache cleanup thread: {e}")

        self.cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        self.cleanup_thread.start()
        logger.info(f"Started cache cleanup thread (interval: {interval_seconds}s)")

    def stop_cleanup_thread(self):
        """åœæ­¢å®šæœŸæ¸…ç†ç·šç¨‹"""
        if self.cleanup_thread:
            # ç·šç¨‹æœƒè‡ªå‹•çµæŸï¼Œå› ç‚ºè¨­ç½®ç‚ºdaemon=True
            self.cleanup_thread = None
            logger.info("Stopped cache cleanup thread")

    def export_cache_index(self, filepath: str = None) -> str:
        """å°å‡ºç·©å­˜ç´¢å¼•"""
        if filepath is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filepath = self.cache_dir / f'cache_index_export_{timestamp}.json'

        try:
            export_data = {
                'export_timestamp': datetime.now().isoformat(),
                'stats': self.get_stats(),
                'entries': {key: asdict(entry) for key, entry in self.cache_index.items()}
            }

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            logger.info(f"Exported cache index to {filepath}")
            return str(filepath)

        except Exception as e:
            logger.error(f"Error exporting cache index: {e}")
            raise

    def import_cache_index(self, filepath: str) -> int:
        """å°å…¥ç·©å­˜ç´¢å¼•"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)

            imported_count = 0
            for key, entry_dict in data.get('entries', {}).items():
                try:
                    entry = CacheEntry(**entry_dict)
                    self.cache_index[key] = entry
                    imported_count += 1
                except Exception as e:
                    logger.error(f"Error importing cache entry {key}: {e}")

            self._save_cache_index()
            logger.info(f"Imported {imported_count} cache entries from {filepath}")
            return imported_count

        except Exception as e:
            logger.error(f"Error importing cache index: {e}")
            return 0


class DataFrameCache:
    """DataFrameå°ˆç”¨ç·©å­˜"""

    def __init__(self, cache_manager: CacheManager):
        """
        åˆå§‹åŒ–DataFrameç·©å­˜

        Args:
            cache_manager: ç·©å­˜ç®¡ç†å™¨å¯¦ä¾‹
        """
        self.cache_manager = cache_manager

    def cache_dataframe(
        self,
        df: pd.DataFrame,
        cache_key: str,
        ttl_seconds: int = None,
        metadata: Dict[str, Any] = None
    ) -> bool:
        """
        ç·©å­˜DataFrame

        Args:
            df: è¦ç·©å­˜çš„DataFrame
            cache_key: ç·©å­˜éµ
            ttl_seconds: ç·©å­˜æ™‚é–“
            metadata: å…ƒæ•¸æ“š

        Returns:
            æ˜¯å¦æˆåŠŸç·©å­˜
        """
        if metadata is None:
            metadata = {}

        # æ·»åŠ DataFrameç‰¹å®šå…ƒæ•¸æ“š
        metadata.update({
            'type': 'dataframe',
            'shape': df.shape,
            'columns': df.columns.tolist(),
            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
            'memory_usage': int(df.memory_usage(deep=True).sum())
        })

        return self.cache_manager.set(cache_key, df, ttl_seconds, metadata)

    def get_dataframe(self, cache_key: str) -> Optional[pd.DataFrame]:
        """
        ç²å–ç·©å­˜çš„DataFrame

        Args:
            cache_key: ç·©å­˜éµ

        Returns:
            ç·©å­˜çš„DataFrameï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å›None
        """
        return self.cache_manager.get(cache_key)

    def cache_query_result(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str,
        df: pd.DataFrame,
        ttl_seconds: int = 1800
    ) -> str:
        """
        ç·©å­˜æŸ¥è©¢çµæœ

        Args:
            symbols: äº¤æ˜“å°ç¬¦è™Ÿåˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ
            df: æŸ¥è©¢çµæœDataFrame
            ttl_seconds: ç·©å­˜æ™‚é–“

        Returns:
            ç·©å­˜éµ
        """
        # å‰µå»ºæŸ¥è©¢ç‰¹å®šçš„ç·©å­˜éµ
        query_key = f"query_{'_'.join(sorted(symbols))}_{start_date}_{end_date}"
        cache_key = hashlib.sha256(query_key.encode()).hexdigest()

        success = self.cache_dataframe(df, cache_key, ttl_seconds, {
            'query_symbols': symbols,
            'query_start_date': start_date,
            'query_end_date': end_date
        })

        return cache_key if success else None

    def get_query_result(
        self,
        symbols: List[str],
        start_date: str,
        end_date: str
    ) -> Optional[pd.DataFrame]:
        """
        ç²å–ç·©å­˜çš„æŸ¥è©¢çµæœ

        Args:
            symbols: äº¤æ˜“å°ç¬¦è™Ÿåˆ—è¡¨
            start_date: é–‹å§‹æ—¥æœŸ
            end_date: çµæŸæ—¥æœŸ

        Returns:
            ç·©å­˜çš„æŸ¥è©¢çµæœï¼Œå¦‚æœä¸å­˜åœ¨å‰‡è¿”å›None
        """
        query_key = f"query_{'_'.join(sorted(symbols))}_{start_date}_{end_date}"
        cache_key = hashlib.sha256(query_key.encode()).hexdigest()

        return self.get_dataframe(cache_key)


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    # å‰µå»ºç·©å­˜ç®¡ç†å™¨
    cache_manager = CacheManager(max_size_gb=0.1, default_ttl=3600)  # 100MBç·©å­˜

    # å‰µå»ºç¯„ä¾‹æ•¸æ“š
    sample_df = pd.DataFrame({
        'timestamp': pd.date_range('2023-01-01', periods=100, freq='1H'),
        'price': np.random.randn(100) + 100,
        'volume': np.random.randint(1000, 10000, 100)
    })

    # ç·©å­˜æ•¸æ“š
    cache_key = "test_dataframe"
    success = cache_manager.set(cache_key, sample_df, ttl_seconds=1800)

    if success:
        print("âœ… Data cached successfully")

        # ç²å–ç·©å­˜æ•¸æ“š
        cached_df = cache_manager.get(cache_key)
        if cached_df is not None:
            print(f"âœ… Retrieved cached data: {len(cached_df)} records")
        else:
            print("âŒ Failed to retrieve cached data")

        # é¡¯ç¤ºçµ±è¨ˆ
        stats = cache_manager.get_stats()
        print(f"ğŸ“Š Cache stats: {stats['total_size_mb']:.2f}MB used, "
              f"{stats['active_entries']} active entries")

    # æ¸…ç†éæœŸæ¢ç›®
    expired_count = cache_manager._cleanup_expired_entries()
    print(f"ğŸ§¹ Cleaned up {expired_count} expired entries")
